# Quiz

## First try

### 01

98% train . 1% dev . 1% test

### 02

Come from the same distribution

### 03

- Make the Neural Network deeper
- Get more training data

### 04

- Increase the regularization parameter lambda
- Get more training data

### 05

A regularization technique (such as L2 regularization) that results in gradient descent shrinking the weights on every iteration.

### 06

Weights are pushed toward becoming smaller (closer to 0)

### 07

You do not apply dropout (do not randomly eliminate units), but keep the 1/keep_prob factor in the calculations used in training.

###08

- Reducing the regularization effect
- Causing the neural network to end up with a lower training set error

### 09

- L2 regularization
- Dropout
- Vanishing gradient
- Exploding gradient

### 10

It makes the cost function faster to optimize

### RÃ©sultat : 5/10

### Faux: 3, 4, 7, 8, 9

## Second try

### 03

- Make the Neural Network deeper
- Increase the number of units in each hidden layer

### 04

- Increase the regularization parameter lambda
- Get more training data

### 07

You do not apply dropout (do not randomly eliminate units) and do not keep the 1/keep_prob factor in the calculations used in training

### 08

- Reducing the regularization effect
- Causing the neural network to end up with a lower training set error

### 09

- L2 regularization
- Dropout
- Data Augmentation