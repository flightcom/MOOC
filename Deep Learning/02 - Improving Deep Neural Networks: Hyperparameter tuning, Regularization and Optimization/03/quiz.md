# Quiz

## First try

### 01
False

### 02
False

### 03
The amount of computational power you can access

### 04
r = np.random.rand()
beta = 1-10**(- r - 1)

### 05
False

### 06
z[l]

### 07
To avoid division by zero

### 08
- β and γ are hyperparameters of the algorithm, which we tune via random sampling.
- They set the mean and variance of the linear variable z[l] of a given layer.
- They can be learned using Adam, Gradient descent with momentum, or RMSprop, not just with gradient descent.

### 09
Perform the needed normalizations, use μ and σ2 estimated using an exponentially weighted average across mini-batches seen during training.

### 10
- Even if a project is currently open source, good governance of the project helps ensure that the it remains open even in the long term, rather than become closed or modified to benefit only one company.

### Result: 8/10
Faux: 8, 10

## Second try

### 08
- They set the mean and variance of the linear variable z[l] of a given layer.
- They can be learned using Adam, Gradient descent with momentum, or RMSprop, not just with gradient descent.

### 10
- Even if a project is currently open source, good governance of the project helps ensure that the it remains open even in the long term, rather than become closed or modified to benefit only one company.
- A programming framework allows you to code up deep learning algorithms with typically fewer lines of code than a lower-level language such as Python.
