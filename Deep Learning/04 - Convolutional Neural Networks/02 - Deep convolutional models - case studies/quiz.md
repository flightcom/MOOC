# QUIZ

## 01
nH and nW decrease, while nC increases

## 02
- Multiple CONV layers followed by a POOL layer
- FC layers in the last few layers

## 03
False

## 04
False

## 05
a[l] and 0, respectively

## 06
- The skip-connection makes it easy for the network to learn an identity mapping between the input and the output within the ResNet block.
- Using a skip-connection helps the gradient to backpropagate and thus helps you to train deeper networks

## 07
17

## 08
- You can use a 1x1 convolutional layer to reduce nC but not nH, nW.
- You can use a pooling layer to reduce nH, nW, but not nC.

## 09
- A single inception block allows the network to use a combination of 1x1, 3x3, 5x5 convolutions and pooling.
- Inception blocks usually use 1x1 convolutions to reduce the input data volumeâ€™s size before applying 3x3 and 5x5 convolutions.

## 10
- Parameters trained for one computer vision task are often useful as pretraining for other computer vision tasks.
- It is a convenient way to get working an implementation of a complex ConvNet architecture.

